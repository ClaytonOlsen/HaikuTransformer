{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merging (32, 47) into a new token 256\n",
      "merging (256, 32) into a new token 257\n",
      "merging (46, 257) into a new token 258\n",
      "merging (101, 32) into a new token 259\n",
      "merging (105, 110) into a new token 260\n",
      "merging (46, 10) into a new token 261\n",
      "merging (32, 116) into a new token 262\n",
      "merging (101, 114) into a new token 263\n",
      "merging (115, 32) into a new token 264\n",
      "merging (116, 32) into a new token 265\n",
      "merging (104, 259) into a new token 266\n",
      "merging (260, 103) into a new token 267\n",
      "merging (111, 110) into a new token 268\n",
      "merging (111, 117) into a new token 269\n",
      "merging (97, 110) into a new token 270\n",
      "merging (121, 32) into a new token 271\n",
      "merging (100, 32) into a new token 272\n",
      "merging (111, 114) into a new token 273\n",
      "merging (101, 97) into a new token 274\n",
      "merging (101, 110) into a new token 275\n",
      "merging (32, 115) into a new token 276\n",
      "merging (262, 266) into a new token 277\n",
      "merging (108, 108) into a new token 278\n",
      "merging (258, 84) into a new token 279\n",
      "merging (101, 115) into a new token 280\n",
      "merging (44, 32) into a new token 281\n",
      "merging (111, 32) into a new token 282\n",
      "merging (97, 114) into a new token 283\n",
      "merging (111, 119) into a new token 284\n",
      "merging (116, 104) into a new token 285\n",
      "merging (267, 32) into a new token 286\n",
      "merging (101, 258) into a new token 287\n",
      "merging (73, 32) into a new token 288\n",
      "merging (104, 97) into a new token 289\n",
      "merging (115, 258) into a new token 290\n",
      "merging (111, 109) into a new token 291\n",
      "merging (115, 116) into a new token 292\n",
      "merging (111, 102) into a new token 293\n",
      "merging (263, 32) into a new token 294\n",
      "merging (108, 105) into a new token 295\n",
      "merging (279, 266) into a new token 296\n",
      "merging (103, 104) into a new token 297\n",
      "merging (104, 105) into a new token 298\n",
      "merging (97, 32) into a new token 299\n",
      "merging (101, 101) into a new token 300\n",
      "merging (258, 65) into a new token 301\n",
      "merging (121, 269) into a new token 302\n",
      "merging (101, 261) into a new token 303\n",
      "merging (114, 97) into a new token 304\n",
      "merging (99, 104) into a new token 305\n",
      "merging (97, 121) into a new token 306\n",
      "merging (105, 116) into a new token 307\n",
      "merging (32, 109) into a new token 308\n",
      "merging (99, 107) into a new token 309\n",
      "merging (97, 116) into a new token 310\n",
      "merging (114, 105) into a new token 311\n",
      "merging (101, 116) into a new token 312\n",
      "merging (105, 114) into a new token 313\n",
      "merging (97, 108) into a new token 314\n",
      "merging (262, 104) into a new token 315\n",
      "merging (63, 257) into a new token 316\n",
      "merging (108, 111) into a new token 317\n",
      "merging (115, 261) into a new token 318\n",
      "merging (101, 272) into a new token 319\n",
      "merging (39, 264) into a new token 320\n",
      "merging (117, 110) into a new token 321\n",
      "merging (117, 115) into a new token 322\n",
      "merging (118, 259) into a new token 323\n",
      "merging (101, 100) into a new token 324\n",
      "merging (32, 119) into a new token 325\n",
      "merging (111, 111) into a new token 326\n",
      "merging (32, 98) into a new token 327\n",
      "merging (97, 278) into a new token 328\n",
      "merging (101, 264) into a new token 329\n",
      "merging (101, 108) into a new token 330\n",
      "merging (117, 114) into a new token 331\n",
      "merging (114, 274) into a new token 332\n",
      "merging (102, 273) into a new token 333\n",
      "merging (258, 73) into a new token 334\n",
      "merging (262, 282) into a new token 335\n",
      "merging (114, 111) into a new token 336\n",
      "merging (258, 83) into a new token 337\n",
      "merging (297, 116) into a new token 338\n",
      "merging (258, 79) into a new token 339\n",
      "merging (108, 97) into a new token 340\n",
      "merging (105, 115) into a new token 341\n",
      "merging (110, 32) into a new token 342\n",
      "merging (262, 111) into a new token 343\n",
      "merging (117, 109) into a new token 344\n",
      "merging (293, 32) into a new token 345\n",
      "merging (73, 39) into a new token 346\n",
      "merging (270, 272) into a new token 347\n",
      "merging (118, 263) into a new token 348\n",
      "merging (101, 109) into a new token 349\n",
      "merging (105, 108) into a new token 350\n",
      "merging (258, 87) into a new token 351\n",
      "merging (111, 108) into a new token 352\n",
      "merging (32, 99) into a new token 353\n",
      "merging (274, 114) into a new token 354\n",
      "merging (111, 112) into a new token 355\n",
      "merging (105, 278) into a new token 356\n",
      "merging (105, 264) into a new token 357\n",
      "merging (117, 116) into a new token 358\n",
      "merging (105, 285) into a new token 359\n",
      "merging (259, 116) into a new token 360\n",
      "merging (110, 111) into a new token 361\n",
      "merging (116, 263) into a new token 362\n",
      "merging (261, 83) into a new token 363\n",
      "merging (110, 284) into a new token 364\n",
      "merging (109, 32) into a new token 365\n",
      "merging (270, 100) into a new token 366\n",
      "merging (105, 100) into a new token 367\n",
      "merging (97, 115) into a new token 368\n",
      "merging (105, 99) into a new token 369\n",
      "merging (261, 84) into a new token 370\n",
      "merging (97, 107) into a new token 371\n",
      "merging (101, 99) into a new token 372\n",
      "merging (258, 70) into a new token 373\n",
      "merging (258, 66) into a new token 374\n",
      "merging (275, 32) into a new token 375\n",
      "merging (258, 77) into a new token 376\n",
      "merging (108, 259) into a new token 377\n",
      "merging (110, 277) into a new token 378\n",
      "merging (298, 264) into a new token 379\n",
      "merging (102, 32) into a new token 380\n",
      "merging (97, 109) into a new token 381\n",
      "merging (276, 116) into a new token 382\n",
      "merging (289, 265) into a new token 383\n",
      "merging (100, 306) into a new token 384\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m haikutext \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(ds[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     12\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m ret()\n\u001b[0;32m---> 13\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhaikutext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ML_folder/HaikuTransformer/tokenizer.py:57\u001b[0m, in \u001b[0;36mRegExpTokenizer.encode\u001b[0;34m(self, text, vocab_size, verbose)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmerging \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpair\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m into a new token \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 57\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# merge common pairs into new token >256\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# save the merge\u001b[39;00m\n\u001b[1;32m     59\u001b[0m merges[pair] \u001b[38;5;241m=\u001b[39m idx\n",
      "File \u001b[0;32m~/ML_folder/HaikuTransformer/tokenizer.py:20\u001b[0m, in \u001b[0;36mmerge\u001b[0;34m(ids, pair, idx)\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     19\u001b[0m     newids\u001b[38;5;241m.\u001b[39mappend(ids[i])\n\u001b[0;32m---> 20\u001b[0m     i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m newids\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "random.seed(1330) \n",
    "import regex as re\n",
    "from tokenizer import RegExpTokenizer as ret\n",
    "\n",
    "ds = load_dataset(\"statworx/haiku\")\n",
    "haikutext = '\\n'.join(ds['train']['text'])\n",
    "tokenizer = ret()\n",
    "tokens = tokenizer.encode(haikutext, 500, verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
